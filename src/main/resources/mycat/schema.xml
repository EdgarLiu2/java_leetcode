<?xml version="1.0"?>
<!DOCTYPE mycat:schema SYSTEM "schema.dtd">
<mycat:schema xmlns:mycat="http://io.mycat/">

    <!-- 定义MyCat的逻辑库 -->
    <!-- 设置表的存储方式. schema name="test_db" 与 server.xml中的test_db 设置一致 -->
    <!-- schema数据库配置
        name：逻辑数据库名，与server.xml中的schema对应。
        checkSQLschema：数据库前缀相关设置，是否检查发给MyCAT的SQL包含数据库名称。如果为true，MyCAT会自动去掉库名
        sqlMaxLimit：select 时默认的limit，避免查询全表。
    -->
    <schema name="test_db" checkSQLschema="false" sqlMaxLimit="1000">
        <!-- 定义MyCat的逻辑表 -->

        <!-- name：逻辑表名 -->
        <!-- dataNode：表数据所存储的数据节点 -->
        <!-- rule：表分片规则 -->
        <table name="t_customer" primaryKey="customer_id" dataNode="customer_dn1,customer_dn2,customer_dn3,customer_dn4" rule="mod-4_customer_id">
        </table>

        <!--t_test表根据 id 进行十进制求模运算-->
        <table name="t_test" primaryKey="id" autoIncrement="true" dataNode="dn1,dn2" rule="mod-long"></table>
        <!--t_task_test表根据org_code列,进行一致性hash规则分片-->
        <table name="t_task_test" dataNode="dn1,dn2" rule="sharding-by-murmur-orgcode"></table>
    </schema>


    <!-- 定义MyCat的数据节点(分片)，存储逻辑表的物理数据库 -->
    <!-- 设置dataNode 对应的数据库，及 mycat 连接的地址dataHost -->
    <dataNode name="customer_dn1" dataHost="mysql-cluster-dh1" database="customer_db"/>
    <dataNode name="customer_dn2" dataHost="mysql-cluster-dh2" database="customer_db"/>
    <dataNode name="customer_dn3" dataHost="mysql-cluster-dh3" database="customer_db"/>
    <dataNode name="customer_dn4" dataHost="mysql-cluster-dh4" database="customer_db"/>



    <!-- MyCat 逻辑主机dataHost对应的物理主机，其中也设置对应的mysql登陆信息 -->
    <!-- 定义数据主机dtHost，连接到MySQL读写分离集群 ,schema中的每一个dataHost中的host属性值必须唯一-->
    <!-- dataHost实际上配置就是后台的数据库集群，一个datahost代表一个数据库集群
        name：唯一标识 dataHost 标签，供dataNode标签使用。
        maxCon：指定每个读写实例连接池的最大连接。也就是说，标签内嵌套的 writeHost、readHost 标签都会使用这个属性的值来实例化出连接池的最大连接数。
        minCon：指定每个读写实例连接池的最小连接，初始化连接池的大小。
    -->
    <!-- 读取负载均衡类型
        balance="0"：不开启读写分离机制，所有读操作都发送到当前可用的 writeHost 上。适合只有一台mysql主库，或者对读写延时非常敏感的应用。
        balance="1"：全部的 readHost 与 stand by writeHost 参与 select 语句的负载均衡，简单的说，当双主双从模式(M1->S1，M2->S2，并且 M1 与 M2 互为主备)，正常情况下，M2,S1,S2 都参与 select 语句的负载均衡。
        balance="2"：所有读操作都随机的在 writeHost、readhost 上分发。适合写数据库负载不大的场景。
        balance="3"：所有读请求随机的分发到 wiriterHost 对应的 readhost 执行，writerHost 不负担读压力。适合一主多从。
    -->
    <!-- 写入负载均衡类型
        writeType="0"：所有写操作发送到配置的第一个 writeHost，这里就是我们的hostmaster，第一个挂了切到还生存的第二个writeHost
        writeType="1"：所有写操作都随机的发送到配置的 writeHost
        writeType="2"：没实现。
    -->
    <!-- switchType 属性
        switchType="-1"：表示不自动切换。
        switchType="1"：默认值，主从自动切换。
        switchType="2"：基于MySQL 主从同步的状态决定是否切换。
    -->
    <dataHost name="mysql-cluster-dh1" maxCon="1000" minCon="10" balance="1"
              writeType="0" dbType="mysql" dbDriver="native" switchType="1" slaveThreshold="100">
        <!-- 心跳检测，检查后端数据库是否可用 -->
        <heartbeat>select user()</heartbeat>

        <!-- 配置后台数据库的IP地址和端口号，还有账号密码 -->
        <writeHost host="mysql-master-db1" url="192.18.0.2:3306" user="root" password="123456">
            <readHost host="mysql-slave-db1" url="192.18.0.3:3306" user="root" password="123456" />
            <readHost host="mysql-slave-db2" url="192.18.0.4:3306" user="root" password="123456" />
        </writeHost>
    </dataHost>
    <dataHost name="mysql-cluster-dh2" maxCon="1000" minCon="10" balance="1"
              writeType="0" dbType="mysql" dbDriver="native" switchType="1" slaveThreshold="100">
        <!-- 心跳检测，检查后端数据库是否可用 -->
        <heartbeat>select user()</heartbeat>

        <!-- 配置后台数据库的IP地址和端口号，还有账号密码 -->
        <writeHost host="mysql-master-db2" url="192.18.0.2:3306" user="root" password="123456">
            <readHost host="mysql-slave-db3" url="192.18.0.3:3306" user="root" password="123456" />
            <readHost host="mysql-slave-db4" url="192.18.0.4:3306" user="root" password="123456" />
        </writeHost>
    </dataHost>
</mycat:schema>